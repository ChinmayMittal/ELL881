{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER)\n",
    "In the last assignment, you tried out using statistical models for NLP tasks.\n",
    "\n",
    "In this assignment you'll be experimenting with deep learning based models for a sequence labelling task: Named Entity Recognition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Task overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we provide a helper function which you can use for reading the data given.\n",
    "\n",
    "Note: Feel free to augment the helper functions given in this notebook as per your need. As long as the overall objective is being met this shouldn't be an issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    with open(filename, \"r\") as file:\n",
    "        text = file.readlines()\n",
    "    return text\n",
    "\n",
    "def process_text(text):\n",
    "    X = []\n",
    "    Y = []\n",
    "    sentenceX = []\n",
    "    sentenceY = []\n",
    "    for line in text:\n",
    "        split = line.split(\" \")\n",
    "        if len(split) > 1:\n",
    "            sentenceX.append(split[0].lower())\n",
    "            sentenceY.append(split[1].replace(\"\\n\", \"\"))\n",
    "        else:\n",
    "            X.append(sentenceX)\n",
    "            Y.append(sentenceY)\n",
    "            sentenceX = []\n",
    "            sentenceY = []\n",
    "    return X, Y\n",
    "\n",
    "text = read_file(\"data/train.txt\")\n",
    "test_text = read_file(\"data/test.txt\")\n",
    "X, Y = process_text(text)\n",
    "X_test, Y_test = process_text(test_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is an example to visualize what is happening here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chancellor O\n",
      "of B-PP\n",
      "the B-NP\n",
      "Exchequer I-NP\n",
      "Nigel B-NP\n",
      "Lawson I-NP\n",
      "'s B-NP\n",
      "restated I-NP\n",
      "commitment I-NP\n",
      "to B-PP\n",
      "a B-NP\n",
      "firm I-NP\n",
      "monetary I-NP\n",
      "policy I-NP\n",
      "has B-VP\n",
      "helped I-VP\n",
      "to I-VP\n",
      "prevent I-VP\n",
      "a B-NP\n",
      "freefall I-NP\n",
      "in B-PP\n",
      "sterling B-NP\n",
      "over B-PP\n",
      "the B-NP\n",
      "past I-NP\n",
      "week I-NP\n",
      ". O\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(X[1])):\n",
    "    print(X[1][i], Y[1][i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how every token in the sentence has been assigned a label. \n",
    "\n",
    "These labels here are being referred to as the named entity.\n",
    "\n",
    "Note that we are following a BIO Tagging scheme here.\n",
    "\n",
    "Basically, every token is either the beginning (B) of a chunk, the continuity of a\n",
    "chunk (I) or outside the chunk (O). E.g. \"Barack Obama went to Greece today\" -> “Barack\n",
    "B-PER Obama I-PER went O to O Greece B-LOC today O.\" Of course, there are other types\n",
    "of tagging schemes also possible like simply BO tagging, where I- is not explicitly tagged,\n",
    "and all contiguous tokens of the same type are combined to extract one entity. In such a\n",
    "schedule, the tagging will be “Barack PER Obama PER went O to O Greece LOC today O.\"\n",
    "\n",
    "You can read more about it [here](https://datascience.stackexchange.com/questions/63399/what-is-bio-tags-for-creating-custom-ner-named-entity-recognization)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task in this assignment is to train a Deep Learning based model using the train set (feel free to split it into train and dev sets) and test out your models performance on a held out dataset (test set). A few points to note are the following:\n",
    "\n",
    "- You need to use deep learning for this assignment. The allowed models are\n",
    "CNN, LSTM. **However, you are NOT allowed to use any pretrained\n",
    "Language Models such as BERT, ELMO, GPT.** You must train all models from\n",
    "scratch. You are allowed to use pre-trained word vectors from word2vec,\n",
    "Glove or FastText. If you wish to use any other pre-trained information, you\n",
    "should ask on Piazza. To avoid confusion, you will be evaluated on the perfomance \n",
    "on one of your models. It would be good if you can show a comparison between the \n",
    "various settings you have tried however implementing one model completely would be \n",
    "sufficient as well. \n",
    "- You may like to create additional features for each token, e.g. whether the\n",
    "token is capitalized or not, whether it’s a number or not etc. You may also try\n",
    "features from lower level syntactic processing like POS tagging or shallow\n",
    "chunking. (This step is optional and meant for your learning).\n",
    "- You are welcome to use probabilistic models like CRF on top of deep learning\n",
    "models. Example, read up on BiLSTM-CRF models for the\n",
    "task of sequence labeling."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: Use of only PyTorch or Tensorflow is allowed for the assignment. PyTorch is recommended. Don't use SpaCy**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we have assumed that the predictions and the true labels are contained in a 1D array as shown below. \n",
    "# If you have a 2D array containing predictions of each sentence in a different array then please first flatten the array so that predictions are contained sequentially.\n",
    "\n",
    "predY_eg = [\"B-NN\", \"O\", \"B-PP\", \"I-PP\"]\n",
    "trueY_eg = [\"B-NN\", \"O\", \"B-PP\", \"B-PP\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THe metrics we'll be using are Micro and Macro F1 scores.\n",
    "\n",
    "You can make use of the following code for calculating the scores which we'll be using for evaluating the performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "[1]\n",
      "Micro F1 score:  0.6666666666666666\n",
      "Macro F1 score:  0.5555555555555555\n",
      "Average F1 score:  0.611111111111111\n"
     ]
    }
   ],
   "source": [
    "def get_scores(predY, trueY):\n",
    "    from sklearn.metrics import f1_score\n",
    "    trueY_O = [i for i, x in enumerate(trueY_eg) if x == \"O\"] ## indices where true value is \"O\"\n",
    "    ### consider only those indices where true value is not 'O'\n",
    "    predY = [predY[i] for i in range(len(predY)) if i not in trueY_O]\n",
    "    trueY = [trueY[i] for i in range(len(trueY)) if i not in trueY_O]\n",
    "\n",
    "    print(\"Micro F1 score: \", f1_score(trueY, predY, average=\"micro\"))\n",
    "    print(\"Macro F1 score: \", f1_score(trueY, predY, average=\"macro\"))\n",
    "    print(\"Average F1 score: \", (f1_score(trueY, predY, average=\"micro\") + f1_score(trueY, predY, average=\"macro\")) / 2)\n",
    "\n",
    "get_scores(predY_eg, trueY_eg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Labels: 22\n",
      "Number of Unique Labels: 19\n"
     ]
    }
   ],
   "source": [
    "def find_unique_labels(Y):\n",
    "    ### Y is a list of lists, each list containts tags\n",
    "    unique_tags = set()\n",
    "    for sentence_predictions in Y:\n",
    "        for tag in sentence_predictions:\n",
    "            unique_tags.add(tag)\n",
    "    print(f\"Number of Unique Labels: {len(unique_tags)}\")\n",
    "\n",
    "    labels_to_ids = {k: v for v, k in enumerate(sorted(unique_tags))}\n",
    "    ids_to_labels = {v: k for v, k in enumerate(sorted(unique_tags))}\n",
    "    return labels_to_ids, ids_to_labels\n",
    "lab_to_id, id_to_label = find_unique_labels(Y)\n",
    "lab_to_id_test, id_to_label_test = find_unique_labels(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Max Seq Length: 78\n",
      "Test Max Seq Length: 70\n"
     ]
    }
   ],
   "source": [
    "def max_seq_len(X):\n",
    "    max_len = 0\n",
    "    for sent in X:\n",
    "        max_len = max(max_len, len(sent))\n",
    "    return max_len\n",
    "\n",
    "print(f\"Train Max Seq Length: {max_seq_len(X)}\")\n",
    "print(f\"Test Max Seq Length: {max_seq_len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtext.legacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchtext\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlegacy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m Field \n\u001b[1;32m      2\u001b[0m text_field \u001b[39m=\u001b[39m Field(\n\u001b[1;32m      3\u001b[0m     sequential \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m      4\u001b[0m     use_vocab \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m     fix_length \u001b[39m=\u001b[39m \u001b[39m80\u001b[39m,\n\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchtext.legacy'"
     ]
    }
   ],
   "source": [
    "from torchtext.legacy.data import Field \n",
    "text_field = Field(\n",
    "    sequential = False,\n",
    "    use_vocab = False,\n",
    "    fix_length = 80,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0847cda728ef3e0f335e7e94b5a043d9a0fda1c620343fc6302f7013063303dc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
